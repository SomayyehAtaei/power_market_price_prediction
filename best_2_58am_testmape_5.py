# -*- coding: utf-8 -*-
"""best_2-58AM_testmape-5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_MgLnzqFGUQiXybQ-rWWRlXVlX1vIzUu
"""



# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import keras
import numpy as np
import tensorflow as tf
# from torchbearer.callbacks import LiveLossPlot
# %matplotlib inline

pd.set_option('max_rows', 99999)

test_percentage = 0.01
val_percentage = 0.05
train_percentage = 1 - (test_percentage + val_percentage)
window_size = 24
batch_size = 50

def windowed_data(data, window_size, batch_size, shuffle_buffer):
  data = tf.data.Dataset.from_tensor_slices(data)
  data = data.window(window_size, shift=1, drop_remainder=True)
  data = data.flat_map(lambda window: window.batch(window_size))
  data = data.map(lambda window: (window[:, :-1], window[-1, -1]))
  data = data.shuffle(buffer_size=1000)
  return data

def windowed_data(data, window_size, sigma):
  data_X = []
  data_y = []

  for i in range(len(data) - window_size):
    x = data[i:i+window_size+1, :-1].tolist()
    data_X.append(x)
    if data[i+window_size, -1] > (3 * sigma):
      data_y.append(3 * sigma)
    else:
      data_y.append(data[i+window_size, -1])
    data_y.append(data[i+window_size, -1])
  return data_X, data_y

def get_primary_targets(data, window_size):
  data_y = []

  for i in range(len(data) - window_size):

    data_y.append(float(data.iloc[i+window_size][-1]))

  return data_y

data = pd.read_csv("/content/drive/MyDrive/mat_data.csv", na_values='nan', keep_default_na=False)
data = data[data['target'] != '']
data.drop(columns=['Unnamed: 0', 'PWSH Price'], inplace=True)
# data.drop(columns=['Hour', 'Weekday', 'SDSH DAP', 'Sgn0 VolumeDir', 'P24HA Price', 'PDSH Price', 'PWA Price'], inplace=True)
data.drop(columns=['Hour', 'Is Working Day', 'Weekday', 'Sgn0 VolumeDir'], inplace=True)
data.head()

import seaborn as sns

sns.set_theme(style="white")
print(data.info())
corr = pd.DataFrame(data, dtype=np.float64).corr()

print(corr)
mask = np.triu(np.ones_like(corr, dtype=bool))

f, ax = plt.subplots(figsize=(11, 9))

cmap = sns.diverging_palette(230, 20, as_cmap=True)

sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

# data_target = data.iloc[:, -1]
# data_dummies = pd.get_dummies(data=data.iloc[:, :-1], columns=['Hour', 'Weekday', 'Is Working Day'])
# data = data_dummies
# data['target'] = data_target
# data.head()

l = len(data)
l_train = int(l * train_percentage)
l_val = int(l * val_percentage)

data_train = data[0:l_train]
data_val = data[l_train:(l_train + l_val)]
data_test = data[(l_train + l_val)+1:]
data_train.head()

train_y_primary = get_primary_targets(data_train, window_size)
val_y_primary = get_primary_targets(data_val, window_size)
test_y_primary = get_primary_targets(data_test, window_size)

data_train_val = pd.concat([data_train, data_val])
 
# ###############################################################################################

# sigma = np.std(data_train_val.astype(float), axis=0)

# for i in range(len(data_train)):
#   for f in range(len(data_train.iloc[i])):
#     if float(data_train.iloc[i, f]) > (3 * sigma[f]):
#       data_train.iloc[i, f] = (3 * sigma[f])
#     elif float(data_train.iloc[i, f]) < (-3 * sigma[f]):
#       data_train.iloc[i, f] = (-3 * sigma[f])

# for i in range(len(data_val)):
#   for f in range(len(data_val.iloc[i])):
#     if float(data_val.iloc[i, f]) > (3 * sigma[f]):
#       data_val.iloc[i, f] = (3 * sigma[f])
#     elif float(data_val.iloc[i, f]) < (-3 * sigma[f]):
#       data_val.iloc[i, f] = (-3 * sigma[f])

# for i in range(len(data_test)):
#   for f in range(len(data_test.iloc[i])):
#     if float(data_test.iloc[i, f]) > (3 * sigma[f]):
#       data_test.iloc[i, f] = (3 * sigma[f])
#     elif float(data_test.iloc[i, f]) < (-3 * sigma[f]):
#       data_test.iloc[i, f] = (-3 * sigma[f])     

# ##############################################################################################

scaler = MinMaxScaler()
data_train_val_scaled = scaler.fit_transform(data_train_val)

data_train_scaled = scaler.transform(data_train)
data_val_scaled = scaler.transform(data_val)
data_test_scaled = scaler.transform(data_test)

###############################################################################################

sigma = np.std(data_train_val_scaled.astype(float), axis=0)

coef = 1.5
for i in range(len(data_train_scaled)):
  for f in range(len(data_train_scaled[i])):
    # if ((float(data_train_scaled[i, f]) > (3 * sigma[f])) or (float(data_train_scaled[i, f]) < (-3 * sigma[f]))):
    if (float(data_train_scaled[i, f]) > (coef * sigma[f])):
      data_train_scaled[i, f] = (coef * sigma[f])
      # data_train_scaled[i, f] = np.mean(data_train_scaled[i-int(window_size/2):i+int(window_size/2), f])
    elif float(data_train_scaled[i, f]) < (-1 * coef * sigma[f]):
      data_train_scaled[i, f] = (-1 * coef * sigma[f])

for i in range(len(data_val_scaled)):
  for f in range(len(data_val_scaled[i])):
    # if ((float(data_val_scaled[i, f]) > (3 * sigma[f])) or (float(data_val_scaled[i, f]) < (-3 * sigma[f]))):
    #   data_val_scaled[i, f] = np.mean(data_val_scaled[i-int(window_size/2):i+int(window_size/2), f])
    if (float(data_val_scaled[i, f]) > (coef * sigma[f])):
      data_val_scaled[i, f] = (coef * sigma[f])
    elif float(data_val_scaled[i, f]) < (-1 * coef * sigma[f]):
      data_val_scaled[i, f] = (-1 * coef * sigma[f])

for i in range(len(data_test_scaled)):
  for f in range(len(data_test_scaled[i])):
    # if ((float(data_test_scaled[i, f]) > (3 * sigma[f])) or (float(data_test_scaled[i, f]) < (-3 * sigma[f]))):
    #   data_test_scaled[i, f] = np.mean(data_test_scaled[i-int(window_size/2):i+int(window_size/2), f])
    if (float(data_test_scaled[i, f]) > (coef * sigma[f])):
      data_test_scaled[i, f] = (coef * sigma[f])
    elif float(data_test_scaled[i, f]) < (-1 * coef * sigma[f]):
      data_test_scaled[i, f] = (-1 * coef * sigma[f])     

##############################################################################################

train_X, train_y = windowed_data(data_train_scaled, window_size=window_size, sigma=sigma)
val_X, val_y = windowed_data(data_val_scaled, window_size=window_size, sigma=sigma)
test_X, test_y = windowed_data(data_test_scaled, window_size=window_size, sigma=sigma)

# train_y = get_primary_targets(data_train, window_size)
# val_y = get_primary_targets(data_val, window_size)
# test_y = get_primary_targets(data_test, window_size)

import matplotlib.pyplot as plt

plt.plot(train_y)
plt.title('train data target')
plt.ylabel('target')
plt.xlabel('t')
plt.legend(['Actual'], loc='upper right')
plt.show()

from keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout, Flatten, Conv2D, GlobalMaxPooling1D
from keras.layers import Input, Dense, LSTM, Activation, concatenate, Attention
from keras.models import Model
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
from keras.callbacks import TensorBoard
# from utils.layer_utils import AttentionLSTM

########################################################################################################################
# Building Network
########################################################################################################################
ip = Input(shape=(window_size+1, len(train_X[0][0])))
# ip = Input(batch_shape=(batch_size, window_size+1, len(train_X[0][0])))

# x = LSTM(8)(ip)
x = LSTM(10, return_sequences=True)(ip)
x = Dropout(0.8)(x)
x = LSTM(16)(x)
x = Dropout(0.8)(x)

y = Permute((2, 1))(ip)
y = Conv1D(16, 8, padding='same', kernel_initializer='glorot_normal')(y)
y = BatchNormalization()(y)
y = Activation('relu')(y)
y = Dropout(0.2)(y)

# y = Conv1D(16, 5, padding='same', kernel_initializer='glorot_normal')(y)
# y = BatchNormalization()(y)
# y = Activation('relu')(y)
# y = Dropout(0.2)(y)

# y = Conv1D(16, 3, padding='same', kernel_initializer='glorot_normal')(y)
# y = BatchNormalization()(y)
# y = Activation('relu')(y)

y = GlobalAveragePooling1D()(y)

x = concatenate([x, y])

out = Dense(1, activation='sigmoid')(x)

model = Model(ip, out)

model.summary()

epochs = 300
learning_rate = 0.0006
c_loss = 'mae'
c_val_loss = 'val_loss'
log_directory = "/content/drive/MyDrive/quant-risk/logs/"
########################################################################################################################
# Train Network
########################################################################################################################
factor = 1. / np.cbrt(2)

model_checkpoint = ModelCheckpoint("/content/drive/MyDrive/quant-risk/Saved Models/", verbose=1, monitor=c_val_loss, save_best_only=True, save_weights_only=False)
reduce_lr = ReduceLROnPlateau(monitor=c_val_loss, patience=5, mode='auto', factor=factor, cooldown=0, min_lr=1e-10, verbose=2)
es = EarlyStopping(monitor=c_val_loss, patience=10)
callback_list = [reduce_lr, es]#[model_checkpoint, reduce_lr, es]

optm = keras.optimizers.Adam(lr=learning_rate)
model.compile(optimizer=optm, loss=c_loss, metrics=['mape'])


history = model.fit(train_X, train_y, batch_size=batch_size,
          epochs=epochs, 
          callbacks=callback_list, verbose=2,
          validation_data=(val_X, val_y))
# history = model.fit(train_X[:(int(len(train_X) / batch_size) * batch_size)], train_y[:(int(len(train_X) / batch_size) * batch_size)], batch_size=batch_size,
#           epochs=epochs, 
#           callbacks=callback_list, verbose=2,
#           validation_data=(val_X[:(int(len(val_X) / batch_size) * batch_size)], val_y[:(int(len(val_X) / batch_size) * batch_size)]))

import matplotlib.pyplot as plt
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper right')
plt.show()
plt.plot(history.history['mape'])
plt.plot(history.history['val_mape'])
plt.title('model mape')
plt.ylabel('mape')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper right')
plt.show()

def invTransform(scaler, data):
  dummy = pd.DataFrame(np.zeros((len(data), len(train_X[0][0])+1)))
  print(data.shape)
  print(dummy.iloc[:, -1].shape)
  dummy.iloc[:, -1] = data
  dummy = pd.DataFrame(scaler.inverse_transform(dummy))
  return dummy.iloc[:, -1]

train_y_pred = model.predict(train_X, batch_size=batch_size)
val_y_pred = model.predict(val_X, batch_size=batch_size)
test_y_pred = model.predict(test_X, batch_size=batch_size)

# train_y_pred = model.predict(train_X[:(int(len(train_X) / batch_size) * batch_size)], batch_size=batch_size)
# val_y_pred = model.predict(val_X[:(int(len(val_X) / batch_size) * batch_size)], batch_size=batch_size)
# test_y_pred = model.predict(test_X[:(int(len(test_X) / batch_size) * batch_size)], batch_size=batch_size)

train_y_final_pred = list(invTransform(scaler, train_y_pred))
val_y_final_pred = list(invTransform(scaler, val_y_pred))
test_y_final_pred = list(invTransform(scaler, test_y_pred))

# train_y_final_pred = train_y_pred
# val_y_final_pred = val_y_pred
# test_y_final_pred = test_y_pred

print(len(train_y_primary))
print(len(val_y_primary))
print(len(test_y_primary))

def calc_mape(y_primary, y_final_pred):
  return (np.mean(abs(np.array(y_primary) - np.array(y_final_pred)) / np.array(y_primary))  * 100)

# def calc_mape(y_primary, y_final_pred):
#   return (np.mean(abs(np.array(y_primary[:len(y_final_pred)]) - np.array(y_final_pred)) / np.array(y_primary[:len(y_final_pred)]))  * 100)

print(calc_mape(train_y_primary, train_y_final_pred))
print(calc_mape(val_y_primary, val_y_final_pred))
print(calc_mape(test_y_primary, test_y_final_pred))

import matplotlib.pyplot as plt

plt.plot(train_y_primary)
plt.plot(train_y_final_pred)
plt.title('train data target')
plt.ylabel('target')
plt.xlabel('t')
plt.legend(['Actual', 'Prediction'], loc='upper right')
plt.show()

plt.plot(val_y_primary)
plt.plot(val_y_final_pred)
plt.title('val data target')
plt.ylabel('target')
plt.xlabel('t')
plt.legend(['Actual', 'Prediction'], loc='upper right')
plt.show()

plt.plot(test_y_primary)
plt.plot(test_y_final_pred)
plt.title('test data target')
plt.ylabel('target')
plt.xlabel('t')
plt.legend(['Actual', 'Prediction'], loc='upper right')
plt.show()

print(test_y_final_pred[:10])
print(test_y_pred[:10])
print(test_y_primary[:10])
print(model.get_weights())